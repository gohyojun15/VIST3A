# VIST3A: Text-to-3D by Stitching a Multi-view Reconstruction Network to a Video Generator


The official code for the paper: "Text-to-3D by Stitching a Multi-view Reconstruction Network to a Video Generator".

>  [Hyojun Go](https://gohyojun15.github.io/), [Dominik Nanhofer](https://scholar.google.com/citations?user=tFx8AhkAAAAJ&hl=en), [Goutam Bhat](https://goutamgmb.github.io/), [Prune Troung](https://prunetruong.com/), [Federico Tombari](https://federicotombari.github.io/), [Konrad Schindler](https://scholar.google.com/citations?user=FZuNgqIAAAAJ&hl=ko).
> 
> ETH Zurich, Google


<!-- <a href="https://arxiv.org/abs/2411.16443"> -->
<img src="https://img.shields.io/badge/arXiv-waiting_for_legal_review-b31b1b.svg"></a>
<!-- <a href="https://gohyojun15.github.io/SplatFlow/"> -->
<img src="https://img.shields.io/badge/Project%20Page-review-brightgreen"></a>


https://github.com/user-attachments/assets/8610f2ac-82cf-4c37-b4e0-6d8d8ff92c6f



## ğŸ”¥ Highlights

VIST3A is a framework for text-to-3D generation that combines a multi-view reconstruction network with a video generator LDM.

- **Text â†’ 3DGS in one LDM path**. Generates high-quality, 3D-consistent Gaussian splats directly from text prompts â€” even with long and detailed descriptions, maintaining both semantic fidelity and visual realism.
- **Models**. Based on Wan 2.1-14B and Wan 2.1-1.3B, we release our own VIST3A-1.3B and VIST3A-14B models.

## ğŸ“¦ Installation
```
TODO:
```

## ğŸš€ Quickstart
```
TODO:
```

## ğŸ§  Training
### ğŸ©¹ Model stitching
```
TODO:
```

### ğŸ¯ Reward Alignment
```
TODO:
```


## ğŸ™ Acknowledgements
We build upon open-source implementations of video LDMs (Wan, CogVideoX, HunyuanVideo, SVD), multi-view recon (AnySplat, VGGT, MVDust3R), and Gsplat. Thanks to the respective authors and communities.
